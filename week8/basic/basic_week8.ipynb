{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PUZv7QhmYCu",
    "outputId": "f9c4b14b-20e1-49dc-da03-cc7cfd2cc383"
   },
   "outputs": [],
   "source": [
    "!pip install trl peft accelerate datasets transformers huggingface_hub wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnGzqpZImgHv"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, default_data_collator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eq_DGD8fnJ_G"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/opt-350m\"\n",
    "DATASET_NAME = \"sahil2801/CodeAlpaca-20k\"\n",
    "DATA_FILE = \"code_alpaca_20k.json\"\n",
    "LORA_RANKS = [8, 128, 256]\n",
    "MAX_SEQ_LENGTH = 128\n",
    "PROJECT_NAME = \"lora_rank_experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286,
     "referenced_widgets": [
      "0de1f4c777204b4fa0ccb47c836db000",
      "517aba380c764013b1752c6d2a310468",
      "1eb6f0166c7e409cac98f23ae9d0c36a",
      "4fef56beb6ab456e910f30181acaa7a6",
      "ace8c2033f5d43d0b1ab2df9b2dab4d6",
      "0eb94b89d6314591b6af13574c262820",
      "67c52d3fcc964ba5a487c4b9de7e7089",
      "5188f06ce1594e66b3693fcd709a2b47",
      "895878b5f6b1478999945dae07428b30",
      "e50b442672284469931e27f99eb903b2",
      "f8ba726afa4d4d11988b607c63a51041",
      "512aa202ce1e408eb269f01aa287e539",
      "d1dbc22b9a334296a26dc409c6a96f40",
      "03404fdf57434931906d07f38fcc12ae",
      "f2cf44d798a342e4974b59ab693427fb",
      "118dbcefdfce4763899f1b929843e999",
      "d17aabaf5962462dadeff1ecd03dba78",
      "988267d3692e474ca1c44c4db84a148e",
      "3ad8c046acb9406c8b576f96be21d202",
      "a26798663b1540ee92b09253bf7b3002",
      "fc04e94e6b204129a38a97c3142584bc",
      "c384b24811b0458bb839ed1b8acf49fa",
      "5bd9382048bc40a38f59e6d067a1ef06",
      "a54790504f3e4ef08239b11d34e9feff",
      "adf75484133444f4aa602f517910ded6",
      "668fe38483a84b1dba898f4611d23d2b",
      "766435206ffe440c9d6fdba0eb944071",
      "09226a86b6784f959ed1d59e29aee1b1",
      "e941d69718544e059f756ca114672dc9",
      "84decb08b1594ca787a6006e8ff4f7aa",
      "b0f18040415d4e16a0b93cd75da26487",
      "63ccacca2a7f494693021bc263138b90",
      "1a09c77e256f40aea7e7be97adbbf42d",
      "119a9e8c99d0414a8ea7a9921c472ca5",
      "01f2e36b3cee4e18918c6ddc1b358dd5",
      "fa79b98615b541f58f03aa125c57e80d",
      "cbfe48c3c490439eb7c8d44695beb059",
      "d75e38cc36eb4d72aa1b3b321835733d",
      "16709a0320b54669a92e48b70744ae79",
      "80f7a9509b2c4b7cbe372f61cb4969c5",
      "0464c10a2b314901a0e02aac33282190",
      "5ef6ff2afc5b48a6afd411347385516c",
      "bb7e5035334b4333b3a75f5a717cd671",
      "35ed5724d3d64cf3b6d7263207b97494",
      "11c3490718f94576a1dcc5200e01fd27",
      "781ce3bd411d41769ee8a0927550ec02",
      "fb71ad35c30e46b38cac8f5f3161285c",
      "2ef9ff1cb035481e8b0649f95e90677a",
      "23018438c7284e3f8ddb208f66486d5f",
      "0e2dced18a5b451ab8e988796ac5ebe3",
      "0b862cdf1a7648edb681d01bf6b0c67b",
      "5a43d422c08841819b561905d417b768",
      "af486aa7df2e4619ace745cea9e3ff7f",
      "abaf5018947648afb438bb778d97f80a",
      "d64ca20f986042139519c79bca714bff"
     ]
    },
    "id": "M5zDBXmQoIJr",
    "outputId": "2d60d57c-bedf-4d12-b596-2b3371605bd2"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "59cac298450644c6b3bd51135138239a",
      "f95b5e7700834be0a20224802fa74973",
      "cc235782aa464e11ad60e0194a824453",
      "f63e782074f74ef9af70f0091c87a18a",
      "f7665954303c439f931e30b4baf2336b",
      "8ada4828b7ae4e9b9d4c7279939ab02b",
      "44ed1eb098124ebe98e851210df1a697",
      "d86040fe91a24993a9f2d8041962c21a",
      "46f4a2fffb204efe9858063b4bdd5b4b",
      "ac4cac727bd1405ab8119403acb2694d",
      "b88a1df16e624bf0afe9a7c18e919e84"
     ]
    },
    "id": "0AfK9FE_oKQg",
    "outputId": "cf7da87e-3a60-44c6-92cd-40ded205028e"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "local_json = hf_hub_download(\n",
    "    repo_id=DATASET_NAME,\n",
    "    filename=DATA_FILE,\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfbSMN0doOfA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "with open(local_json, 'r', encoding='utf-8') as f:\n",
    "    records = json.load(f)\n",
    "records = records[:1000]\n",
    "raw_dataset = Dataset.from_list(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jx9HFjTqpq3",
    "outputId": "fca2dc45-c222-45e1-c0ff-73590312ed90"
   },
   "outputs": [],
   "source": [
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDz5OVSxs2Y0",
    "outputId": "8bd7c4c9-264b-4ff4-fb2b-213af1c7629a"
   },
   "outputs": [],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "krlkFZd6pLr3",
    "outputId": "53cb64f4-80c2-45ee-e11d-78cd9417a6f5"
   },
   "outputs": [],
   "source": [
    "raw_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "13f239fb14404c29a544472ce1380efd",
      "b40dcfc3c2a241028d907017ff6bef1c",
      "a1f08b86194944ef8c4439951c38e0aa",
      "ecffc7d78ae34749b11fcceab5449187",
      "5dc218c2c5fb4e58b321a5d23071a835",
      "00d982a947d64bca9fe76a402b1dbfc5",
      "0e9e6513fd07461b928e2eec9fb44538",
      "cc0192c6f33d43b3a16ca9b8291b7874",
      "d8a2c0595cc8433d815c246b918dc292",
      "eda5727252e349f0bda52ac479b1a827",
      "f30f33fe76624d4590dbe280938b01fd"
     ]
    },
    "id": "W67UAscrpkVG",
    "outputId": "f9a67510-576a-4d45-f1d9-54e1cf9267a1"
   },
   "outputs": [],
   "source": [
    "def prepare_data(examples, tokenizer, max_length=512):\n",
    "    \"\"\"데이터 전처리 함수\"\"\"\n",
    "    # 프롬프트와 응답을 결합\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(\n",
    "        examples[\"instruction\"],\n",
    "        examples[\"input\"],\n",
    "        examples[\"output\"]\n",
    "    ):\n",
    "        # 입력이 있는 경우와 없는 경우 구분\n",
    "        if input_text:\n",
    "            text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
    "        else:\n",
    "            text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "        texts.append(text)\n",
    "\n",
    "    # 토크나이징\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # SFT 형식에 맞게 데이터 구성\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": tokenized[\"input_ids\"].clone()\n",
    "    }\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    lambda x: prepare_data(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I88lDoGVtbLe",
    "outputId": "4f0f067a-0a3e-4aa1-b673-4773e6bc1b39"
   },
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cIsl7uyyXRA"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NOtiSbLqAA7"
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s-ovCx3Yp4GO",
    "outputId": "605d225a-67e4-4ea0-b942-4e00dba0368d"
   },
   "outputs": [],
   "source": [
    "for r in LORA_RANKS:\n",
    "    run_name = f\"lora_r_{r}\"\n",
    "    wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        name=run_name,\n",
    "        reinit=True,\n",
    "        config={\n",
    "            \"lora_rank\": r,\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "            \"learning_rate\": 2e-4,\n",
    "            \"batch_size\": 4,\n",
    "            \"gradient_accumulation_steps\": 4,\n",
    "            \"num_epochs\": 3\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16\n",
    "    ).cuda()\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_time = time.time()\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        args=SFTConfig(\n",
    "            output_dir=os.path.join(\"./results\", run_name),\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "            learning_rate=2e-4,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=100,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"no\",  # 저장 비활성화\n",
    "            eval_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            disable_tqdm=False,\n",
    "            label_names=[\"labels\"],\n",
    "            fp16=True,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"wandb\"\n",
    "        ),\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    peak_memory_gb = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    steps_per_sec = trainer.state.global_step / duration if duration > 0 else 0.0\n",
    "\n",
    "    wandb.log({\n",
    "        \"duration_sec\": duration,\n",
    "        \"peak_memory_gb\": peak_memory_gb,\n",
    "        \"steps_per_sec\": steps_per_sec,\n",
    "        \"final_loss\": trainer.state.log_history[-1][\"train_loss\"] if trainer.state.log_history else None,\n",
    "        \"total_steps\": trainer.state.global_step,\n",
    "    })\n",
    "\n",
    "    model.save_pretrained(os.path.join(\"./results\", run_name))\n",
    "    # wandb에 모델 아티팩트로 저장\n",
    "    artifact = wandb.Artifact(\n",
    "        name=f\"model-lora-r-{r}\",\n",
    "        type=\"model\",\n",
    "        description=f\"LoRA model with rank {r}\"\n",
    "    )\n",
    "    artifact.add_dir(os.path.join(\"./results\", run_name))\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# 모든 실험이 끝난 후 wandb에 요약 리포트 생성\n",
    "wandb.init(project=PROJECT_NAME, name=\"experiment_summary\", reinit=True)\n",
    "for r in LORA_RANKS:\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(f\"{wandb.run.entity}/{PROJECT_NAME}\", filters={\"name\": f\"lora_r_{r}\"})\n",
    "    if runs:\n",
    "        run = runs[0]\n",
    "        wandb.log({\n",
    "            f\"rank_{r}_final_loss\": run.summary.get(\"final_loss\"),\n",
    "            f\"rank_{r}_duration\": run.summary.get(\"duration_sec\"),\n",
    "            f\"rank_{r}_memory\": run.summary.get(\"peak_memory_gb\"),\n",
    "            f\"rank_{r}_speed\": run.summary.get(\"steps_per_sec\")\n",
    "        })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpznhW5Fq6kQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
