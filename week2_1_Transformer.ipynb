{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymxatB5WYxlL"
      },
      "source": [
        "# 📌[2주차/기본] 주어진 문장에서 나올 다음 단어를 예측하는 모델 구현\n",
        "\n",
        "이번 과제에서는 Transformer를 last word prediction이라는 task에 적용합니다.\n",
        "Last word prediction은 Token list가 주어졌을 때, 다음으로 오는 token을 예측하는 task로, 추후 등장할 LLM의 핵심입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1X7RM2du1zcr",
        "outputId": "f091f9f5-b8ab-4877-ce8a-4f22707a69a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "foKuFNzizgPj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📄 데이터 로딩\n",
        "\n",
        "IMDb (Internet Movie database)\n",
        "\n",
        "train 25000개 와 test 25000개로 이루어져 있지만 이 중 5%만 사용한다.\n",
        "\n",
        "#### 🔍 라벨\n",
        "0 : 부정적인 리뷰\n",
        "1 : 긍정적인 리뷰\n",
        "\n",
        "#### 📦 샘플\n",
        "```python\n",
        "{\n",
        "  'text': \"I watched this movie with my family and we all hated it. The story was weak...\",\n",
        "  'label': 0\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "-_B0J6kbzC3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\")\n",
        "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test[:5%]\")"
      ],
      "metadata": {
        "id": "ojgWEo47yma2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔤 tokenizer 설정\n",
        "\n",
        "huggingface에서 bert-base-uncased 모델을 불러와서 설정을 한다.\n",
        "\n",
        "* 토큰화 방식\n",
        " * BERT : 단어를 서브워드 단위로 분해\n",
        " * BPE : 자주 등장하는 문자쌍을 병합\n",
        " * SentencePiece : 공백까지 포함해서 분해\n",
        "\n",
        "* 대소문자 처리\n",
        " * uncased : 모두 소문자로 치환\n",
        " * cased : 대소문자 유지\n",
        "\n"
      ],
      "metadata": {
        "id": "qi7hDLau0AlG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HOdhoBVA1zcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e43e31-0fa5-47cc-a747-1fa512ef8350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📦 collate_fn\n",
        "\n",
        "text 데이터는 샘플마다 문장 길이가 다르기 때문에 padding이 필요하다\n",
        "* tokenizer\n",
        " * padding=True : 짧은 문장도 padding을 사용해서 일정한 길이로 맞춰준다..\n",
        " * truncation : 너무 긴 문장은 max_len 기준으로 자름\n",
        "\n",
        "🌟 기존 감정 분류와 달라진 점\n",
        "\n",
        "기본 과제에선 label에 input_ids[-2] 를 넣어주고 있는데, 이는 마지막 단어 예측이 우리 모델의 목표이기 때문이다.\n"
      ],
      "metadata": {
        "id": "TpHn4zAs3aF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[-2])\n",
        "    texts.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[:-2]))\n",
        "\n",
        "  texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "pjXRfZhc23X6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-FshZcTZBQ2"
      },
      "source": [
        "## ✏️ Self-attention\n",
        "\n",
        "이번에는 self-attention을 구현해보겠습니다.\n",
        "Self-attention은 shape이 (B, S, D)인 embedding이 들어왔을 때 attention을 적용하여 새로운 representation을 만들어내는 module입니다.\n",
        "\n",
        "여기서 B는 batch size, S는 sequence length, D는 embedding 차원입니다.\n",
        "\n",
        "* forward에서 필요한 입력\n",
        " * x : 입력 시퀀스\n",
        " * mask : 패딩 토큰 무시하거나 casual attention 하는 경우 필요\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "vFUlZn_h22jD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.wq = nn.Linear(input_dim, d_model) #in : (B,S,D) output: (B,S,d_model)\n",
        "        self.wk = nn.Linear(input_dim, d_model)\n",
        "        self.wv = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        q = self.wq(x) #(B,S,d_model)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "\n",
        "        score= torch.matmul(q, k.transpose(-1,-2)) # 뒤에서 첫번째 두번째를 바꿔준다 k.transpose = (B,d_model,S) score = (B,S,S)\n",
        "        score = score / sqrt(self.d_model)\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score + (mask * -1e9)\n",
        "\n",
        "        score = self.softmax(score)\n",
        "        result = torch.matmul(score, v)\n",
        "        result = self.dense(result)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "FZRsgrYwAgnq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S0vMp85ZRNO"
      },
      "source": [
        "대부분 transformer 구조에 대해 그냥 구현한 것에 불과하지만 mask는 새로운 개념\n",
        "\n",
        "단어와 단어 사이의 관계도를 보는 Attention score에서 실제 단어와 패딩 단어 사이의 관계도를 고려할 필요가 전혀 없기 때문에 -1e-9를 더하여, 소프트맥스에서 확률 0이 출력되도록 만든 것입니다"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 🧱 TransformerLayer\n",
        "\n",
        "* Self-Attention : 이미 구현해둔 class 사용\n",
        "* Feed-Forward Network (FFN) : MLP 구조 (Linear > ReLU > Linear)\n",
        "  * 각 토큰 하나하나에 대해 독립적으로 처리하는 fully connected network\n",
        "\n",
        "\n",
        "🧠 요약\n",
        "\n",
        "Attention : token간 관계 표현 개선\n",
        "\n",
        "FFN : 각 토큰 자체의 표현 개선\n"
      ],
      "metadata": {
        "id": "v9F2SmnljYrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VZHPCn9AS5Gp"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "\n",
        "    self.sa = SelfAttention(input_dim, d_model)\n",
        "    self.ffn = nn.Sequential(\n",
        "      nn.Linear(d_model, dff),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dff, d_model)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sa(x, mask)\n",
        "    x = self.ffn(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3VYrqTJagS1"
      },
      "source": [
        "## 🧮 Positional encoding\n",
        "\n",
        "* 순서 정보를 인코딩해서 모델에 넣어주는 기술\n",
        "* 각 위치마다 고유한 벡터를 만들어서 입력 임베딩에 더해주는 방식\n",
        "* Transformer는 Attention 구조라서 문장을 볼때 병렬로 처리하기 때문에 RNN 처럼 순차적 처리가 되지 않음. 따라서 명시적으로 알려줘야 한다.\n",
        "\n",
        "```python\n",
        "x = token_embedding + positional_encoding\n",
        "```\n",
        "\n",
        "이번에는 positional encoding을 구현합니다. Positional encoding의 식은 다음과 같습니다:\n",
        "$$\n",
        "\\begin{align*} PE_{pos, 2i} &= \\sin\\left( \\frac{pos}{10000^{2i/D}} \\right), \\\\ PE_{pos, 2i+1} &= \\cos\\left( \\frac{pos}{10000^{2i/D}} \\right).\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf_jMQWDUR79",
        "outputId": "10d6f172-efab-4451-99e4-f89bdceb5459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400, 256])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400\n",
        "print(positional_encoding(max_len, 256).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 🔧 LastWordPredictor (basic 과제)\n",
        "\n",
        "####⁉️ 변경 사항\n",
        "1. 단어 예측 task인 만큼 tokenizer의 vocab_size로 최종 출력 차원을 조정해준다.\n",
        "2. 문장의 첫번째 토큰 벡터를 뽑는 코드를 삭제하고 마지막 토큰을 뽑도록 한다\n",
        "\n",
        "####⁉️ sqrt(d_model) 하는 이유\n",
        "embedding된 벡터는 작은 값인데, positional encoding은 값이 커서 positional encoding의 영향이 너무 큼 > scale up\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 TextClassifier (실습 내용)\n",
        "\n",
        "1. 입력 토큰을 임베딩 + 포지셔널 인코딩\n",
        "2. 여러개의 TransformerLayer 통과\n",
        "3. 문장의 첫번째 토큰의 벡터를 뽑아서\n",
        "4. 최종 classification\n",
        "\n",
        "####⁉️ 첫번째 토큰 벡터 뽑는 이유\n",
        "Embedding, positional encoding, transformer layer를 거치고 난 후 마지막 label을 예측하기 위해 사용한 값은 x[:, 0]입니다. 기존의 RNN에서는 padding token을 제외한 마지막 token에 해당하는 representation을 사용한 것과 다릅니다. 이렇게 사용할 수 있는 이유는 attention 과정을 보시면 첫 번째 token에 대한 representation은 이후의 모든 token의 영향을 받습니다. 즉, 첫 번째 token 또한 전체 문장을 대변하는 의미를 가지고 있다고 할 수 있습니다. 그래서 일반적으로 Transformer를 text 분류에 사용할 때는 이와 같은 방식으로 구현됩니다."
      ],
      "metadata": {
        "id": "sSgvgIkImkBM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8MaiCGh8TsDH"
      },
      "outputs": [],
      "source": [
        "class LastWordPredictor(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model) # d_model의 차원의 벡터로 embedding\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)]) # n_layer만큼 transformerLayer를 쌓음\n",
        "    self.classification = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #패딩 토큰을 찾아서 mask 생성\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :] # (B,1,S)\n",
        "\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = x * sqrt(self.d_model)\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, -1]\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = LastWordPredictor(len(tokenizer), 32, 2, 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDq05OlAb2lB"
      },
      "source": [
        "## 학습\n",
        "\n",
        "#### 🔧 변경사항\n",
        "* loss_fn 이 BCEWithLigitsLoss 는 이진 분류를 위한 것 이므로 변경\n",
        "* preds 구하는 법과 label을 정수형으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "r88BALxO1zc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    preds = torch.argmax(preds, dim=-1)\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "al_b56TYRILq"
      },
      "outputs": [],
      "source": [
        "def model_train(model, n_epochs):\n",
        "    train_acc_list = []\n",
        "    test_acc_list = []\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        model.train()\n",
        "        for data in train_loader:\n",
        "            model.zero_grad()\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "            preds = model(inputs)\n",
        "            loss = loss_fn(preds, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_losses.append(total_loss)\n",
        "        print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            train_acc = accuracy(model, train_loader)\n",
        "            test_acc = accuracy(model, test_loader)\n",
        "            train_acc_list.append(train_acc)\n",
        "            test_acc_list.append(test_acc)\n",
        "            print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n",
        "    return train_losses, train_acc_list, test_acc_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "##hyperparam\n",
        "n_epochs = 50\n",
        "lr = 0.001\n",
        "\n",
        "model = LastWordPredictor(len(tokenizer), 32, 2, 32)\n",
        "model = model.to('cuda')\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_losses, train_acc_list, test_acc_list = model_train(model, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_GMfB1t03ZO",
        "outputId": "232f17b9-ce04-4a9f-98c2-d8151a81462c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 192.90383625030518\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   1 | Train Loss: 103.57606196403503\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   2 | Train Loss: 69.05914735794067\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   3 | Train Loss: 55.8815176486969\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   4 | Train Loss: 51.567612051963806\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   5 | Train Loss: 49.205313205718994\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   6 | Train Loss: 47.639344334602356\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   7 | Train Loss: 46.623597383499146\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   8 | Train Loss: 45.67187321186066\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch   9 | Train Loss: 45.51940643787384\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch  10 | Train Loss: 45.07234513759613\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch  11 | Train Loss: 44.79322838783264\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch  12 | Train Loss: 43.99995970726013\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch  13 | Train Loss: 43.83711361885071\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch  14 | Train Loss: 43.59956157207489\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch  15 | Train Loss: 44.11829054355621\n",
            "=========> Train acc: 0.584 | Test acc: 0.612\n",
            "Epoch  16 | Train Loss: 43.049736976623535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(train_losses, train_accs, test_accs):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Loss plot\n",
        "    ax1.plot(epochs, train_losses, marker='o', linestyle='-', color='blue')\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2.plot(epochs, train_accs, marker='o', linestyle='-', label='Train Accuracy', color='green')\n",
        "    ax2.plot(epochs, test_accs, marker='x', linestyle='--', label='Test Accuracy', color='red')\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "u-I121osywgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(train_losses, train_acc_list, test_acc_list)"
      ],
      "metadata": {
        "id": "rNd9va67yizb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💭 결과\n",
        "* loss가 거의 일정한 수준 유지\n",
        "* train test accuracy도 전혀 개선되지 않음\n",
        "* 학습이 제대로 되지 않는 것 같아서 개선할 점 찾아야함\n",
        "\n",
        "### 🚩 의심가는 점\n",
        "* 마지막 토큰 뽑는 x[:-1] 에서 padding 고려 안함\n",
        "* embedding 차원 증가시켜서 성능 개선할 수 있을지\n",
        "* 아니면 fail나는거 한번 까보기\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "RC5D1qhnzneV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🧪 실험1\n",
        "패딩 고려한 진짜 마지막 단어 찾기\n",
        "\n",
        "```\n",
        "[\n",
        "  [단어1, 단어2, 단어3, PAD, PAD],     # 문장 길이 3\n",
        "  [단어4, 단어5, 단어6, 단어7, PAD],   # 문장 길이 4\n",
        "  [단어8, 단어9, PAD, PAD, PAD]        # 문장 길이 2\n",
        "]\n",
        "```\n",
        "x[:-1] 에서는 PAD 쪽 토큰이 추출된다."
      ],
      "metadata": {
        "id": "GP3H_Kzg1zI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LastWordPredictor_edit_padding(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model) # d_model의 차원의 벡터로 embedding\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff) for _ in range(n_layers)]) # n_layer만큼 transformerLayer를 쌓음\n",
        "    self.classification = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #패딩 토큰을 찾아서 mask 생성\n",
        "    padd_mask = (x == tokenizer.pad_token_id)\n",
        "    mask = padd_mask[:, None, :] # (B,1,S)\n",
        "\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = x * sqrt(self.d_model)\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    lengths = (~padd_mask).sum(dim=1) #각 문장의 실제 길이 리스트\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    last_token_output = x[torch.arange(batch_size), lengths -1, :]\n",
        "    x = self.classification(last_token_output)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "g8lCOyTn0XBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_padding = LastWordPredictor_edit_padding(len(tokenizer), 32, 2, 32)\n",
        "model_with_padding = model_with_padding.to('cuda')\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model_with_padding.parameters(), lr=lr)\n",
        "\n",
        "train_losses2, train_acc_list2, test_acc_list2 = model_train(model_with_padding, n_epochs)"
      ],
      "metadata": {
        "id": "3rp87oeW27h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔍 데이터 확인"
      ],
      "metadata": {
        "id": "mVvuiHd64j1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "label_counter = Counter()\n",
        "\n",
        "for texts, labels in train_loader:\n",
        "    tokens = tokenizer.convert_ids_to_tokens(labels.numpy())\n",
        "    label_counter.update(tokens)\n",
        "\n",
        "print(\"가장 많이 등장한 레이블 상위 10개:\")\n",
        "for token, freq in label_counter.most_common(10):\n",
        "    print(f\"{token:<10}: {freq}회\")\n"
      ],
      "metadata": {
        "id": "WpteYRea4IWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qalQD1V4Ir0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}